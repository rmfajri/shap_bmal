{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import model_selection,naive_bayes,svm\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "import re\n",
    "from utils import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from math import log\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import shap\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the text\n",
    "#Make to function, preprocess, and to calculate cosine similarity\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    non_word='[^a-zA-Z]'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    parsed_text=re.sub(non_word,' ',parsed_text)\n",
    "    return parsed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter setting\n",
    "C = 1\n",
    "max_iter = 1000\n",
    "Degree=3\n",
    "Gamma='auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"dataset/Founta_edit/train.csv\")\n",
    "test=pd.read_csv(\"dataset/Founta_edit/test.csv\")\n",
    "unlabel=pd.read_csv(\"dataset/Founta_edit/unlabel.csv\")\n",
    "gold_standard=pd.read_csv(\"dataset/Founta_edit/gold_standard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)\n",
    "test.dropna(inplace=True)\n",
    "unlabel.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2369\n",
      "2371\n"
     ]
    }
   ],
   "source": [
    "#mulai dari sini\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training a SVM classifier and predict on unlabel pool\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text'])\n",
    "x_train = Tfidf_vect.transform(train['text']).toarray()\n",
    "x_test = Tfidf_vect.transform(test['text']).toarray()\n",
    "x_pool=Tfidf_vect.transform(unlabel['text']).toarray()\n",
    "y_train=train['label']\n",
    "print(\"model prediction and predict on unlabel\")\n",
    "model = SVC( max_iter=max_iter, C=C, kernel='linear',probability=True,gamma=Gamma,degree=Degree)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_train,y_train)\n",
    "probs=model.predict_proba(x_pool)\n",
    "entropy=list()\n",
    "for p in probs:\n",
    "    ent=0\n",
    "    ent1=-p[0] * log(p[0],2)\n",
    "    ent2=-p[1] * log(p[1],2)\n",
    "    ent=ent1+ent2\n",
    "    entropy.append(ent)\n",
    "df=pd.DataFrame({'index':unlabel['index'],\"text\":unlabel['text'],\"confidence\":entropy})\n",
    "df=df.sort_values(by=\"confidence\",ascending=False)\n",
    "unlabel_length=len(unlabel)*0.1\n",
    "low_conf=df[0:round(unlabel_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate shapley value on low conf data\n",
    "x_pool=Tfidf_vect.transform(low_conf['text']).toarray()\n",
    "print(\"calculate Shapley value \")\n",
    "explainer = shap.LinearExplainer(model, x_train,feature_dependence=\"independent\")\n",
    "shap_values_train = explainer.shap_values(x_train)\n",
    "shap_values_pool = explainer.shap_values(x_pool)\n",
    "n_clusters=20\n",
    "\n",
    "print(\"clusterint \")\n",
    "kmeans = KMeans(n_clusters= n_clusters, n_jobs=-1, max_iter=600)\n",
    "kmeans.fit(shap_values_pool)\n",
    "\n",
    "#calculate the center\n",
    "similarity_to_center = list()\n",
    "similarity=0\n",
    "for i, instance in enumerate(shap_values_pool):\n",
    "    cluster_label = kmeans.labels_[i] # cluster of this instance\n",
    "    centroid = kmeans.cluster_centers_[cluster_label] # cluster center of the cluster of that instance\n",
    "    #similarity = 1-cosine(instance, centroid) # 1- cosine distance gives similarity\n",
    "    similarity = (1)-(distance.cosine([instance], [centroid]))\n",
    "    similarity_to_center.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inver=Tfidf_vect.inverse_transform(shap_values_pool)\n",
    "#sentence=[' '.join(item) for item in inver]\n",
    "#sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label=pd.DataFrame(kmeans.labels_)\n",
    "cluster_labels=pd.DataFrame({\"index\":low_conf['index'],\"text\":low_conf['text'],'cluster':kmeans.labels_,\n",
    "                            'similarity':similarity_to_center,'uncertainty':low_conf['confidence']})\n",
    "cluster_dict=dict()\n",
    "n_cluster=20\n",
    "#creating the dataframe for each cluster\n",
    "for item in range(0,n_cluster):\n",
    "    cluster_dict['cluster_{0}'.format(item)]=cluster_labels[cluster_labels['cluster']==item] \n",
    "#Create the dataframe for each cluster\n",
    "for i in range(0,n_cluster):\n",
    "    globals()['cluster_{}'.format(i)] = pd.DataFrame(cluster_dict['cluster_{}'.format(i)])\n",
    "#calculate the center of the cluster\n",
    "#get the center of the cluster\n",
    "center=kmeans.cluster_centers_\n",
    "#print(center.shape)\n",
    "#create the data frame of the cluster center\n",
    "centerpd=pd.DataFrame(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the value on each cluster based on similarity\n",
    "for i in range(0,n_cluster):\n",
    "    globals()['cluster_{}'.format(i)]=globals()['cluster_{}'.format(i)].sort_values(by='uncertainty',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,n_cluster):\n",
    "    print(len(globals()['cluster_{}'.format(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cluster 5\")\n",
    "print(\"number of samples = {}\".format(len(cluster_6)))\n",
    "print(\"max_sim = {}\".format(cluster_6['similarity'].max()))\n",
    "print(\"min_sim = {}\".format(cluster_6['similarity'].min()))\n",
    "print(\"sim_range={}\".format(cluster_6['similarity'].max()-cluster_6['similarity'].min()))\n",
    "\n",
    "print(\"===================================================\")\n",
    "print(\"max_unc = {}\".format(cluster_6['uncertainty'].max()))\n",
    "print(\"min_unc = {}\".format(cluster_6['uncertainty'].min()))\n",
    "print(\"unc_range={}\".format(cluster_6['uncertainty'].max()-cluster_6['uncertainty'].min()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_11.to_csv(\"bad_cluster2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the uncertainty of data points in csv file\n",
    "#for i in range(0,n_cluster):\n",
    "#    name_file=\"uncertainty/8/cluster_{0}.csv\".format(i)\n",
    "#    globals()['cluster_{}'.format(i)].to_csv(name_file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude the bad cluster\n",
    "cluster_exclude=list()\n",
    "for i in range(0,20):\n",
    "    if len(globals()['cluster_{}'.format(i)])>=50:\n",
    "        cluster_exclude.append(i)\n",
    "        del globals()['cluster_{}'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label propagation\n",
    "#read the ground truth\n",
    "search_idx=list()\n",
    "gold_standard=pd.read_csv(\"result/gold_standard.csv\")\n",
    "for i in range(0,20):\n",
    "    if i in cluster_exclude:\n",
    "        continue\n",
    "    else:\n",
    "        #search for the label in the ground truth\n",
    "        search_index=globals()['cluster_{}'.format(i)]['index'].head(1).values[0]\n",
    "        labeled_selection=gold_standard.loc[gold_standard['index']==search_index]\n",
    "        labels=labeled_selection['label'].values[0]\n",
    "        globals()['cluster_{}'.format(i)]['label']=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data and balance it before put back in the training dataset\n",
    "print(\"Balancing \")\n",
    "conf_data=pd.DataFrame()\n",
    "for i in range(0,20):\n",
    "    if i in cluster_exclude:\n",
    "        continue\n",
    "    else:\n",
    "        conf_data=conf_data.append(globals()['cluster_{}'.format(i)])\n",
    "print(len(conf_data[conf_data['label']==0]))\n",
    "print(len(conf_data[conf_data['label']==1]))\n",
    "majority=conf_data[conf_data['label']==0]\n",
    "n_minority_class=conf_data[conf_data['label']==1]\n",
    "if len(n_minority_class) > len(majority):\n",
    "    cluster_size=len(majority)\n",
    "elif len(n_minority_class)==0:\n",
    "    cluster_size=1\n",
    "else:\n",
    "    cluster_size=len(n_minority_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the majority and cluster it based on number of minority class\n",
    "embeddings = Tfidf_vect.transform(majority['text'])\n",
    "#initialize the kmeans\n",
    "kmeans = KMeans(n_clusters=cluster_size, random_state=0,max_iter=600,n_init=10)\n",
    "kmeans.fit(embeddings)\n",
    "labels = (kmeans.labels_)\n",
    "#create the cluster for each low conf data\n",
    "cluster_labels = pd.DataFrame({'index':majority['index'],'cluster':labels,'text':majority['text'],\n",
    "                               'label':majority['label']})\n",
    "cluster_dict=dict()\n",
    "#creating the dataframe for each cluster\n",
    "for item in range(0,cluster_size):\n",
    "    cluster_dict['cluster_{0}'.format(item)]=cluster_labels[cluster_labels['cluster']==item] \n",
    "for i in range(0,cluster_size):\n",
    "        globals()['cluster_{}'.format(i)] = pd.DataFrame(cluster_dict['cluster_{}'.format(i)])\n",
    "center=kmeans.cluster_centers_\n",
    "#print(center.shape)\n",
    "#create the data frame of the cluster center\n",
    "centerpd=pd.DataFrame(center)\n",
    "#inverse transform the center to text\n",
    "inver=Tfidf_vect.inverse_transform(centerpd)\n",
    "#get the inverse word and create as a sentence\n",
    "sentence=[' '.join(item) for item in inver]\n",
    "#Calculate the data that close to the center for each cluster\n",
    "for i in range(0,cluster_size):\n",
    "    sent=list()\n",
    "    cossim=list()\n",
    "    for item in globals()['cluster_{}'.format(i)]['text']:\n",
    "        sent.append(item)\n",
    "    for k in sent:\n",
    "        vector1=text_to_vector(k)\n",
    "        vector2=text_to_vector(sentence[i])\n",
    "        cosine=get_cosine(vector1,vector2)\n",
    "        cossim.append(cosine)\n",
    "    globals()['cluster_{}'.format(i)]['similarity']=cossim\n",
    "    globals()['cluster_{}'.format(i)]=globals()['cluster_{}'.format(i)].sort_values(by='similarity',ascending=False)\n",
    "center_data=pd.DataFrame()\n",
    "for i in range(0,cluster_size):\n",
    "    center_data=center_data.append(globals()['cluster_{}'.format(i)].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding to training data\n",
    "train_1=n_minority_class\n",
    "train_0=center_data\n",
    "new_train=pd.DataFrame()\n",
    "#adding the balanced set for training\n",
    "frames=[train_1,train_0] \n",
    "\n",
    "new_train=pd.concat(frames)\n",
    "new_train.drop(['cluster','similarity'],axis=1,inplace=True)\n",
    "training_data=pd.DataFrame({'index':new_train['index'],\n",
    "                           'label':new_train['label'],'text':new_train['text']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"minorty class {}\".format(len(new_train[new_train['label']==1])))\n",
    "print(\"majority class {}\".format(len(new_train[new_train['label']==0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to training data and remove from unlabel pool\n",
    "train=train.append(training_data)\n",
    "train.to_csv(\"training/training.csv\",index=False)\n",
    "\n",
    "test.to_csv(\"training/test.csv\",index=False)\n",
    "\n",
    "unlabel=unlabel[~unlabel['index'].isin(train['index'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating performance\n",
    "train1=pd.read_csv(\"training/training.csv\")\n",
    "test1=pd.read_csv(\"training/test.csv\")\n",
    "\n",
    "train1.dropna(inplace=True)\n",
    "test1.dropna(inplace=True)\n",
    "\n",
    "\n",
    "X_train=train1['text']\n",
    "y_train=train1['label']\n",
    "X_test=test1['text']\n",
    "y_test=test1['label']\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(train['text'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test)\n",
    "SVM = svm.SVC(C=1, kernel='linear', gamma='auto',probability=True)\n",
    "SVM.fit(Train_X_Tfidf,y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "score=geometric_mean_score(y_test, predictions_SVM)\n",
    "#gmean.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
